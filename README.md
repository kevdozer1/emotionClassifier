
Project by Kevin Hopkins

This project revolves around multimodal emotion classification, incorporating data from three distinct modalities: vision, speech, and text. The goal is to detect and categorize human emotions into four primary classes, utilizing an extensive array of data processing, feature extraction, and machine learning techniques. Different methods for pooling and temporal modeling are employed to address the temporal dimension in audio and visual files. Various classifiers are applied for emotion classification, with the most effective parameters selected via Grid Search. The project also addresses class imbalance and applies both early and late fusion techniques to yield optimal results. A thorough evaluation and interpretation of the unimodal and multimodal classification tasks informs better performance.
